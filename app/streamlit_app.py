import streamlit as st
import pandas as pd
from utils.openai_api import get_llm_response

# Optional: metrics if response exists
from eval.readability_metrics import flesch_score, coleman_liau_index, word_count
from eval.response_scoring import step_count

# --- App Config ---
st.set_page_config(page_title="Tutor LLM Evaluation Dashboard", layout="wide")
st.title("üß† Interactive Tutor Evaluation Dashboard")
with st.expander("‚ÑπÔ∏è About this App"):
    st.markdown("""
    This interactive dashboard demonstrates how large language models (LLMs), such as GPT-3.5 and GPT-4o, respond to middle school‚Äìlevel math questions under different prompting strategies: zero-shot, few-shot, and chain-of-thought. 

    The tool is designed to support research and experimentation in educational AI, prompt engineering, and model evaluation. Users can explore how different prompts affect the structure, readability, and depth of responses generated by the models.

    Each response is automatically assessed using readability indices, word count, and reasoning step analysis to facilitate structured comparison. This app is intended for educators, edtech developers, and AI researchers interested in adapting foundation models for instructional use.
    """)

# --- Load Questions ---
@st.cache_data
def load_questions(csv_path):
    return pd.read_csv(csv_path)

question_df = load_questions("data/student_questions.csv")

# --- Sidebar Controls ---
st.sidebar.header("üìö Select a Question")
selected_question = st.sidebar.selectbox("Choose a question:", question_df["question_text"].tolist())

st.sidebar.header("üß† Prompt Strategy")
prompt_strategy = st.sidebar.radio("Prompt Type", ["Zero-shot", "Few-shot", "Chain-of-thought"])

st.sidebar.header("ü§ñ Language Model")
selected_model = st.sidebar.radio("LLM Model", ["gpt-3.5-turbo", "gpt-4o"])

# --- Load Prompt Template ---
def load_prompt_template(strategy):
    path_map = {
        "Zero-shot": "prompts/zero_shot.txt",
        "Few-shot": "prompts/few_shot.txt",
        "Chain-of-thought": "prompts/cot.txt"
    }
    try:
        with open(path_map[strategy], "r") as f:
            return f.read()
    except FileNotFoundError:
        return "Prompt template not found."

template = load_prompt_template(prompt_strategy)
final_prompt = template.format(question=selected_question)

# --- Display Prompt ---
st.markdown("### üìù Prompt Sent to LLM")
st.code(final_prompt, language="text")

# --- Generate LLM Response ---
if st.button("üí¨ Generate Response"):
    with st.spinner("Contacting OpenAI..."):
        response_text = get_llm_response(final_prompt, model=selected_model)
        st.markdown("### ü§ñ LLM Response")
        st.write(response_text)

        # Save to session for evaluation
        st.session_state["response_text"] = response_text

# --- Evaluation Metrics ---
if "response_text" in st.session_state:
    response = st.session_state["response_text"]
    st.markdown("### üìè Evaluation Metrics")
    st.write(f"- **Readability (Flesch)**: {flesch_score(response):.2f}")
    st.write(f"- **Coleman-Liau Index**: {coleman_liau_index(response):.2f}")
    st.write(f"- **Word Count**: {word_count(response)}")
    st.write(f"- **Step Count**: {step_count(response)}")

st.markdown("---")
st.caption("Built by Alex Wasdahl ‚Äî explore how LLMs explain math.")
