# -*- coding: utf-8 -*-
"""openai_api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PIB-JuzeQO0c0Qsp9oVF81kVP06yErDs
"""

pip install python-dotenv

import openai
import os

# Optionally load from .env
from dotenv import load_dotenv
load_dotenv()

# Set API key
openai.api_key = os.getenv("OPENAI_API_KEY")

def get_llm_response(prompt, model="gpt-3.5-turbo", temperature=0.3):
    """
    Sends a prompt to the specified OpenAI chat model and returns the response.

    Args:
        prompt (str): The prompt text to send.
        model (str): The OpenAI model to use (e.g., "gpt-3.5-turbo", "gpt-4o").
        temperature (float): Sampling temperature.

    Returns:
        str: The generated response text.
    """
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature
        )
        return response.choices[0].message["content"].strip()
    except Exception as e:
        return f"‚ùå Error fetching response: {e}"